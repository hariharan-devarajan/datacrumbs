{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis for DataCrumbs\n",
    "\n",
    "This is a simple analysis notebook for Datacrumbs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "import dask\n",
    "import os\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import math\n",
    "import zindex_py as zindex\n",
    "import numpy as np\n",
    "import intervals as I\n",
    "import pandas as pd\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster, progress, wait, get_client\n",
    "from dask.distributed import Future, get_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_root = str(Path(os.getcwd()).parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "    ],\n",
    "    format=\"%(asctime)s [%(levelname)s]: %(message)s in %(pathname)s:%(lineno)d\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Dask Local Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers=16\n",
    "cluster = LocalCluster(n_workers=workers)  # Launches a scheduler and workers locally\n",
    "client = Client(cluster)  # Connect to distributed cluster and override default\n",
    "logging.info(f\"Initialized Client with {workers} workers and link {client.dashboard_link}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'venv (Python 3.9.12)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "import os \n",
    "is_trace = True\n",
    "folder=\"/usr/workspace/haridev/xio/\"\n",
    "file=f\"{folder}/logs/ior/*.pfw.gz\"\n",
    "file_pattern = glob(file)\n",
    "file_pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to load trace data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(filename):\n",
    "    index_file = f\"{filename}.zindex\"\n",
    "    if not os.path.exists(index_file):\n",
    "        status = zindex.create_index(filename, index_file=f\"file:{index_file}\",\n",
    "                                     regex=\"id:\\b([0-9]+)\", numeric=True, unique=True, debug=False, verbose=False)\n",
    "        logging.debug(f\"Creating Index for {filename} returned {status}\")\n",
    "    return filename\n",
    "\n",
    "def get_linenumber(filename):\n",
    "    index_file = f\"{filename}.zindex\"\n",
    "    line_number = zindex.get_max_line(filename, index_file=index_file, debug=False, verbose=False)\n",
    "    logging.debug(f\" The {filename} has {line_number} lines\")\n",
    "    return (filename, line_number)\n",
    "\n",
    "def get_size(filename):\n",
    "    if filename.endswith('.pfw'):\n",
    "        size = os.stat(filename).st_size\n",
    "    elif filename.endswith('.pfw.gz'):\n",
    "        index_file = f\"{filename}.zindex\"\n",
    "        line_number = zindex.get_max_line(filename, index_file=index_file,debug=False, verbose=False)\n",
    "        size = line_number * 256\n",
    "    logging.debug(f\" The {filename} has {size/1024**3} GB size\")\n",
    "    return int(size)\n",
    "\n",
    "\n",
    "def generate_line_batches(filename, max_line):\n",
    "    batch_size = 16*1024\n",
    "    for start in range(0, max_line, batch_size):\n",
    "        end =  min((start + batch_size - 1) , (max_line - 1))\n",
    "        logging.debug(f\"Created a batch for {filename} from [{start}, {end}] lines\")\n",
    "        yield filename, start, end\n",
    "\n",
    "def load_indexed_gzip_files(filename, start, end):\n",
    "    index_file = f\"{filename}.zindex\"\n",
    "    json_lines = zindex.zquery(filename, index_file=index_file,\n",
    "                          raw=f\"select a.line from LineOffsets a where a.line >= {start} AND a.line <= {end};\", debug=False, verbose=False)\n",
    "    logging.debug(f\"Read {len(json_lines)} json lines for [{start}, {end}]\")\n",
    "    return json_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_profile(line):\n",
    "    d = {}\n",
    "    if line is not None and line !=\"\" and len(line) > 0 and \"[\" != line[0] and line != \"\\n\" :\n",
    "        try:\n",
    "            unicode_line = ''.join([i if ord(i) < 128 else '#' for i in line])\n",
    "            val = json.loads(unicode_line)\n",
    "            if \"pid\" in d:\n",
    "                d[\"pid\"] = val[\"pid\"]\n",
    "            if \"tid\" in d:\n",
    "                d[\"tid\"] = val[\"tid\"]\n",
    "            if \"ts\" in d:\n",
    "                d[\"ts_us\"] = int(val[\"ts\"])\n",
    "            d[\"filename\"] = \"NA\"\n",
    "            if \"args\" in val:\n",
    "                if \"time\" in val[\"args\"]:\n",
    "                    d[\"dur_sec\"] = float(val[\"args\"][\"time\"])\n",
    "                if \"freq\" in val[\"args\"]:\n",
    "                    d[\"freq\"] = val[\"args\"][\"freq\"]\n",
    "                if \"size_sum\" in val[\"args\"]:\n",
    "                    d[\"size_bytes\"] = val[\"args\"][\"size_sum\"]\n",
    "                if \"fname\" in val[\"args\"] and val[\"args\"][\"fname\"]:\n",
    "                    d[\"filename\"] = val[\"args\"][\"fname\"]\n",
    "            d[\"func_id\"] = val[\"name\"]\n",
    "            d[\"cat\"] = val[\"cat\"]\n",
    "        except Exception as error:\n",
    "            logging.error(f\"Processing {line} failed with {error}\")\n",
    "    return d\n",
    "\n",
    "\n",
    "def load_trace(line):\n",
    "    d = {}\n",
    "    if line is not None and line !=\"\" and len(line) > 0 and \"[\" != line[0] and line != \"\\n\" :\n",
    "        try:\n",
    "            unicode_line = ''.join([i if ord(i) < 128 else '#' for i in line])\n",
    "            val = json.loads(unicode_line)\n",
    "            d[\"name\"] = val[\"name\"]\n",
    "            d[\"cat\"] = val[\"cat\"]\n",
    "            if \"pid\" in val:\n",
    "                d[\"pid\"] = val[\"pid\"]\n",
    "            if \"tid\" in val:\n",
    "                d[\"tid\"] = val[\"tid\"]\n",
    "            d[\"ts\"] = 0\n",
    "            d[\"dur\"] = 0\n",
    "            if \"ts\" in val:\n",
    "                d[\"ts\"] = int(val[\"ts\"])\n",
    "                d[\"te\"] = int(val[\"ts\"])\n",
    "            d[\"dur\"] = 1\n",
    "            if \"dur\" in val:\n",
    "                d[\"dur\"] = int(val[\"dur\"])\n",
    "            if \"args\" in val and \"hhash\" in val[\"args\"]:                    \n",
    "                d[\"hhash\"] = val[\"args\"][\"hhash\"]\n",
    "            if \"ts\" in val:\n",
    "                interval = I.closedopen(d[\"ts\"], d[\"ts\"] + 1)\n",
    "                if d[\"dur\"] > 0:\n",
    "                    d[\"te\"] = int(val[\"ts\"]) + d[\"dur\"]\n",
    "                    interval = I.closedopen(d[\"ts\"], d[\"ts\"] + d[\"dur\"])\n",
    "                d[\"interval\"] = I.to_string(interval)\n",
    "            if val[\"ph\"] != \"M\":\n",
    "                d[\"type\"] = 0    \n",
    "                if \"args\" in val:                    \n",
    "                    if \"hhash\" in val[\"args\"]:\n",
    "                        d[\"hhash\"] = val[\"args\"][\"hhash\"]\n",
    "                    if \"size_sum\" in val[\"args\"]:\n",
    "                        d[\"size\"] = val[\"args\"][\"size_sum\"]\n",
    "                    if \"fhash\" in val[\"args\"]:\n",
    "                        d[\"fhash\"] = val[\"args\"][\"fhash\"]\n",
    "            else:\n",
    "                if val[\"name\"] == \"FH\":\n",
    "                    d[\"type\"] = 1\n",
    "                    if \"args\" in val:\n",
    "                        if \"name\" in val[\"args\"]:\n",
    "                            d[\"name\"] = val[\"args\"][\"name\"]\n",
    "                        if \"value\" in val[\"args\"]:\n",
    "                            d[\"hash\"] = val[\"args\"][\"value\"]\n",
    "                elif val[\"name\"] == \"HH\":\n",
    "                    d[\"type\"] = 2\n",
    "                    if \"args\" in val:\n",
    "                        if \"name\" in val[\"args\"]:\n",
    "                            d[\"name\"] = val[\"args\"][\"name\"]\n",
    "                        if \"value\" in val[\"args\"]:\n",
    "                            d[\"hash\"] = val[\"args\"][\"value\"]\n",
    "            \n",
    "        except Exception as error:\n",
    "            logging.error(f\"Processing {line} failed with {error}\")\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dask Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(file_pattern) > 0:\n",
    "    dask.bag.from_sequence(file_pattern).map(create_index).compute()\n",
    "    logging.info(f\"Created index for {len(file_pattern)} files\")\n",
    "    total_size = dask.bag.from_sequence(file_pattern).map(get_size).sum()\n",
    "    n_partition = math.ceil(total_size.compute() / (128 * 1024 ** 2))\n",
    "    logging.info(f\"Total size of all files are {total_size} bytes\")\n",
    "    max_line_numbers = dask.bag.from_sequence(file_pattern).map(get_linenumber).compute()\n",
    "    logging.info(f\"Max lines per file are {max_line_numbers}\")\n",
    "    json_line_delayed = []\n",
    "    total_lines = 0\n",
    "    for filename, max_line in max_line_numbers:\n",
    "        total_lines += max_line\n",
    "        for _, start, end in generate_line_batches(filename, max_line):\n",
    "            json_line_delayed.append((filename, start, end))\n",
    "\n",
    "    logging.info(f\"Loading {len(json_line_delayed)} batches out of {len(file_pattern)} files and has {total_lines} lines overall\")\n",
    "    json_line_bags = []\n",
    "    for filename, start, end in json_line_delayed:\n",
    "        num_lines = end - start + 1\n",
    "        json_line_bags.append(dask.delayed(load_indexed_gzip_files, nout=num_lines)(filename, start, end))\n",
    "    json_lines = dask.bag.concat(json_line_bags)\n",
    "    if is_trace:\n",
    "        pfw_bag = json_lines.map(load_trace).filter(lambda x: \"name\" in x)\n",
    "    else:\n",
    "        pfw_bag = json_lines.map(load_profile).filter(lambda x: \"func_id\" in x)\n",
    "    pfw_bag.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_trace:\n",
    "    columns = {'hhash': \"string[pyarrow]\", 'pid': \"uint64[pyarrow]\", 'tid': \"uint64[pyarrow]\",\n",
    "                'cat': \"string[pyarrow]\", 'name': \"string[pyarrow]\", 'type':  \"uint8[pyarrow]\",\n",
    "            'ts': \"uint64[pyarrow]\", 'te': \"uint64[pyarrow]\", 'dur': \"uint64[pyarrow]\", 'interval': \"string[pyarrow]\", \n",
    "             'size': \"uint64[pyarrow]\", 'fhash': \"string[pyarrow]\", 'hash': \"string[pyarrow]\", \n",
    "           }\n",
    "else:\n",
    "    columns = {'pid': \"uint64[pyarrow]\", 'tid': \"uint64[pyarrow]\",\n",
    "            'ts_us': \"uint64[pyarrow]\", 'dur_sec': \"float32[pyarrow]\", \n",
    "            'freq': \"uint64[pyarrow]\", 'size_bytes': \"uint64[pyarrow]\", 'name': \"string[pyarrow]\", \n",
    "            'filename': \"string[pyarrow]\", \n",
    "            'cat': \"string[pyarrow]\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = pfw_bag.to_dataframe(meta=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = events.repartition(npartitions=n_partition).persist()\n",
    "_ = wait(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhash = events.query(\"type == 1\")[[\"name\",\"hash\"]]\n",
    "hhash = events.query(\"type == 2\")[[\"name\",\"hash\"]]\n",
    "event = events.query(\"type == 0\")\n",
    "fhashes = fhash.query(\"name.str.contains('test.bat')\").compute()[\"hash\"]\n",
    "fhashes = fhashes.to_list()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "interesting_events = event.query(\"fhash.isin(@value)\", local_dict={\"value\": fhashes}).sort_values(\"ts\")\n",
    "interesting_events.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_events[\"combined_name\"] = interesting_events[\"name\"] + \"-\" + interesting_events[\"cat\"]\n",
    "ts_events = interesting_events[[\"size\"]].compute().reset_index().drop(\"index\", axis=1)\n",
    "ts_events[\"size\"].fillna(value=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_intervals  = interesting_events[[\"interval\",\"name\"]].compute()\n",
    "interesting_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# interesting_events[\"interval\"] = interesting_events.apply(lambda x: I.to_string(I.closed(x[\"ts\"], x[\"ts\"]+x[\"dur\"])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_func(df):\n",
    "    val = I.empty()\n",
    "    for index, value in df.items():\n",
    "        if str(value) != 'NA':\n",
    "            pad_interval = I.from_string(str(value), int)\n",
    "            val = val.union(pad_interval)\n",
    "    logging.debug(f\"Grouped Range into {val}\")\n",
    "    return I.to_string(val)\n",
    "def union_portions():\n",
    "    return dd.Aggregation(\n",
    "        'union_portions',\n",
    "        chunk=lambda s: s.apply(group_func),\n",
    "        agg=lambda s: s.apply(group_func)\n",
    "    )\n",
    "relevant_intervals = interesting_events[[\"interval\"]].reduction(chunk=lambda s: s.apply(group_func), aggregate=lambda s1: s1.apply(group_func))[\"interval\"].compute()\n",
    "relevant_intervals = I.from_string(relevant_intervals, int)\n",
    "relevant_intervals_list = list(relevant_intervals)\n",
    "relevant_intervals_list[:10], len(relevant_intervals_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_ts = relevant_intervals_list[0].lower\n",
    "max_te = relevant_intervals_list[-1].upper\n",
    "min_ts, max_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_events = event.query(f\"ts >= {min_ts - 1e5} and te <= {max_te + 1e5} and dur > 0\")\n",
    "filtered_events.compute()\n",
    "# filtered_events = event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_events[\"combined_name\"] = filtered_events[\"name\"] + \"-\" + filtered_events[\"cat\"]\n",
    "event_batch_per_sys_call = []\n",
    "rows = list(interesting_intervals.iterrows())\n",
    "count = 0\n",
    "ops_map = {}\n",
    "ops_counter = 0\n",
    "for index, row in tqdm(rows):\n",
    "    interval = I.from_string(row[\"interval\"], int)\n",
    "    ops = row[\"name\"]\n",
    "    a_overlaps_b = f\"(ts >= {interval.lower} and ts <=  {interval.upper}) or (te >= {interval.lower} and te <=  {interval.upper})\"\n",
    "    b_overlaps_a = f\"({interval.lower} >= ts and {interval.lower} <=  te) or ({interval.upper} >= ts and {interval.upper} <=  te)\"\n",
    "    batch = filtered_events.query(f\"{a_overlaps_b} or {b_overlaps_a}\")[[\"combined_name\", \"dur\"]].groupby(\"combined_name\").sum().compute()\n",
    "    if ops in ops_map:\n",
    "        op_value = ops_map[ops]\n",
    "    else:\n",
    "        ops_counter += 1\n",
    "        op_value = ops_counter\n",
    "        ops_map[ops] = op_value\n",
    "    batch.loc['op'] = [op_value]\n",
    "    event_batch_per_sys_call.append(batch)\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = None\n",
    "count = 0\n",
    "for batch in tqdm(event_batch_per_sys_call):\n",
    "    if merged_df is not None:\n",
    "        merged_df = merged_df.merge(batch, how='outer', on=\"combined_name\",suffixes=('', f\"_{count}\"))\n",
    "    else:\n",
    "        merged_df = batch\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = merged_df.transpose()\n",
    "df = dataset.reset_index().drop(\"index\", axis=1)\n",
    "df[\"op_name\"] = \"UNKNOWN\"\n",
    "for key, value in ops_map.items():\n",
    "    df[\"op_name\"] = df[\"op_name\"].mask(df[\"op\"].eq(value), key)\n",
    "df[\"op_name\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = df.merge(ts_events, how='outer', left_index=True, right_index=True)\n",
    "final_dataset[\"BW\"] = final_dataset[f\"size\"] / (1024**2) / (final_dataset[f\"{ops}-sys\"] / 1e9)\n",
    "final_dataset.drop([f\"{ops}-sys\", \"size\"], inplace=True, axis=1)\n",
    "final_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file\n",
    "final_dataset[\"op\"] = ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset.to_json(path_or_buf=f\"{output_file}\",orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file=\"/usr/workspace/haridev/xio/output/jslines/write_ops-64_ts-64m-RAW-DIRECT.pfw.gz.jsonl\"\n",
    "ops=\"write\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "files = glob(f\"{output_file}\")\n",
    "final_dataset_l = []\n",
    "for file in files:\n",
    "    final_dataset_l.append(dd.read_json(file))\n",
    "final_dataset = dd.concat(final_dataset_l).compute().reset_index().drop(\"index\", axis=1)\n",
    "final_dataset[\"BW\"] = final_dataset[f\"transfer_size\"] / (1024**2) / (final_dataset[f\"{ops}-sys\"]/1e9)\n",
    "final_dataset[\"op\"] = ops\n",
    "final_dataset.to_json(path_or_buf=f\"{output_file}\",orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make the number of relevance features dynamic.\n",
    "\n",
    "1. Add up the importance score to reach 95%.\n",
    "2. Add Transfer size\n",
    "3. Split features into layers and do this analysis per layer.\n",
    "4. Correlation\n",
    "   1. correlation matrix.\n",
    "   2. PCA\n",
    "   3. Lasso Regression (L1)\n",
    "   4. Auto regression\n",
    "5. SHAPLEY value (feature importance)\n",
    "   1. Tree SHAP\n",
    "6. How portable are the interfaces (do not overfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tanzima for better models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## models\n",
    "- sequential training: gradient boost\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
