{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis for DataCrumbs\n",
    "\n",
    "This is a simple analysis notebook for Datacrumbs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "import dask\n",
    "import os\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import math\n",
    "import zindex_py as zindex\n",
    "import numpy as np\n",
    "import intervals as I\n",
    "import pandas as pd\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster, progress, wait, get_client\n",
    "from dask.distributed import Future, get_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_root = str(Path(os.getcwd()).parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "    ],\n",
    "    format=\"%(asctime)s [%(levelname)s]: %(message)s in %(pathname)s:%(lineno)d\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Dask Local Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 12:37:37,089 [INFO]: Initialized Client with 16 workers and link http://127.0.0.1:8787/status in /var/tmp/haridev/ipykernel_359281/3142773904.py:4\n"
     ]
    }
   ],
   "source": [
    "workers=16\n",
    "cluster = LocalCluster(n_workers=workers)  # Launches a scheduler and workers locally\n",
    "client = Client(cluster)  # Connect to distributed cluster and override default\n",
    "logging.info(f\"Initialized Client with {workers} workers and link {client.dashboard_link}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/usr/workspace/haridev/xio//logs/ior/ior_4k.pfw.gz',\n",
       " '/usr/workspace/haridev/xio//logs/ior/ior_16k-64k.pfw.gz',\n",
       " '/usr/workspace/haridev/xio//logs/ior/ior-1mb-4mb.pfw.gz',\n",
       " '/usr/workspace/haridev/xio//logs/ior/ior-256k.pfw.gz',\n",
       " '/usr/workspace/haridev/xio//logs/ior/ior-256k-multiple-interfaces.pfw.gz',\n",
       " '/usr/workspace/haridev/xio//logs/ior/ior-hdf5-256k.pfw.gz']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os \n",
    "is_trace = True\n",
    "folder=\"/usr/workspace/haridev/xio/\"\n",
    "file=f\"{folder}/logs/ior/*.pfw.gz\"\n",
    "file_pattern = glob(file)\n",
    "file_pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to load trace data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(filename):\n",
    "    index_file = f\"{filename}.zindex\"\n",
    "    if not os.path.exists(index_file):\n",
    "        status = zindex.create_index(filename, index_file=f\"file:{index_file}\",\n",
    "                                     regex=\"id:\\b([0-9]+)\", numeric=True, unique=True, debug=False, verbose=False)\n",
    "        logging.debug(f\"Creating Index for {filename} returned {status}\")\n",
    "    return filename\n",
    "\n",
    "def get_linenumber(filename):\n",
    "    index_file = f\"{filename}.zindex\"\n",
    "    line_number = zindex.get_max_line(filename, index_file=index_file, debug=False, verbose=False)\n",
    "    logging.debug(f\" The {filename} has {line_number} lines\")\n",
    "    return (filename, line_number)\n",
    "\n",
    "def get_size(filename):\n",
    "    if filename.endswith('.pfw'):\n",
    "        size = os.stat(filename).st_size\n",
    "    elif filename.endswith('.pfw.gz'):\n",
    "        index_file = f\"{filename}.zindex\"\n",
    "        line_number = zindex.get_max_line(filename, index_file=index_file,debug=False, verbose=False)\n",
    "        size = line_number * 256\n",
    "    logging.debug(f\" The {filename} has {size/1024**3} GB size\")\n",
    "    return int(size)\n",
    "\n",
    "\n",
    "def generate_line_batches(filename, max_line):\n",
    "    batch_size = 16*1024\n",
    "    for start in range(0, max_line, batch_size):\n",
    "        end =  min((start + batch_size - 1) , (max_line - 1))\n",
    "        logging.debug(f\"Created a batch for {filename} from [{start}, {end}] lines\")\n",
    "        yield filename, start, end\n",
    "\n",
    "def load_indexed_gzip_files(filename, start, end):\n",
    "    index_file = f\"{filename}.zindex\"\n",
    "    json_lines = zindex.zquery(filename, index_file=index_file,\n",
    "                          raw=f\"select a.line from LineOffsets a where a.line >= {start} AND a.line <= {end};\", debug=False, verbose=False)\n",
    "    logging.debug(f\"Read {len(json_lines)} json lines for [{start}, {end}]\")\n",
    "    return json_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_profile(line):\n",
    "    d = {}\n",
    "    if line is not None and line !=\"\" and len(line) > 0 and \"[\" != line[0] and line != \"\\n\" :\n",
    "        try:\n",
    "            unicode_line = ''.join([i if ord(i) < 128 else '#' for i in line])\n",
    "            val = json.loads(unicode_line)\n",
    "            if \"pid\" in d:\n",
    "                d[\"pid\"] = val[\"pid\"]\n",
    "            if \"tid\" in d:\n",
    "                d[\"tid\"] = val[\"tid\"]\n",
    "            if \"ts\" in d:\n",
    "                d[\"ts_us\"] = int(val[\"ts\"])\n",
    "            d[\"filename\"] = \"NA\"\n",
    "            if \"args\" in val:\n",
    "                if \"time\" in val[\"args\"]:\n",
    "                    d[\"dur_sec\"] = float(val[\"args\"][\"time\"])\n",
    "                if \"freq\" in val[\"args\"]:\n",
    "                    d[\"freq\"] = val[\"args\"][\"freq\"]\n",
    "                if \"size_sum\" in val[\"args\"]:\n",
    "                    d[\"size_bytes\"] = val[\"args\"][\"size_sum\"]\n",
    "                if \"fname\" in val[\"args\"] and val[\"args\"][\"fname\"]:\n",
    "                    d[\"filename\"] = val[\"args\"][\"fname\"]\n",
    "            d[\"func_id\"] = val[\"name\"]\n",
    "            d[\"cat\"] = val[\"cat\"]\n",
    "        except Exception as error:\n",
    "            logging.error(f\"Processing {line} failed with {error}\")\n",
    "    return d\n",
    "\n",
    "\n",
    "def load_trace(line):\n",
    "    d = {}\n",
    "    if line is not None and line !=\"\" and len(line) > 0 and \"[\" != line[0] and line != \"\\n\" :\n",
    "        try:\n",
    "            unicode_line = ''.join([i if ord(i) < 128 else '#' for i in line])\n",
    "            val = json.loads(unicode_line)\n",
    "            d[\"name\"] = val[\"name\"]\n",
    "            d[\"cat\"] = val[\"cat\"]\n",
    "            if \"pid\" in val:\n",
    "                d[\"pid\"] = val[\"pid\"]\n",
    "            if \"tid\" in val:\n",
    "                d[\"tid\"] = val[\"tid\"]\n",
    "            d[\"ts\"] = 0\n",
    "            d[\"dur\"] = 0\n",
    "            if \"ts\" in val:\n",
    "                d[\"ts\"] = int(val[\"ts\"])\n",
    "                d[\"te\"] = int(val[\"ts\"])\n",
    "            d[\"dur\"] = 1\n",
    "            if \"dur\" in val:\n",
    "                d[\"dur\"] = int(val[\"dur\"])\n",
    "            if \"args\" in val and \"hhash\" in val[\"args\"]:                    \n",
    "                d[\"hhash\"] = val[\"args\"][\"hhash\"]\n",
    "            if \"ts\" in val:\n",
    "                interval = I.closedopen(d[\"ts\"], d[\"ts\"] + 1)\n",
    "                if d[\"dur\"] > 0:\n",
    "                    d[\"te\"] = int(val[\"ts\"]) + d[\"dur\"]\n",
    "                    interval = I.closedopen(d[\"ts\"], d[\"ts\"] + d[\"dur\"])\n",
    "                d[\"interval\"] = I.to_string(interval)\n",
    "            if val[\"ph\"] != \"M\":\n",
    "                d[\"type\"] = 0    \n",
    "                if \"args\" in val:                    \n",
    "                    if \"hhash\" in val[\"args\"]:\n",
    "                        d[\"hhash\"] = val[\"args\"][\"hhash\"]\n",
    "                    if \"size_sum\" in val[\"args\"]:\n",
    "                        d[\"size\"] = val[\"args\"][\"size_sum\"]\n",
    "                    if \"fhash\" in val[\"args\"]:\n",
    "                        d[\"fhash\"] = val[\"args\"][\"fhash\"]\n",
    "            else:\n",
    "                if val[\"name\"] == \"FH\":\n",
    "                    d[\"type\"] = 1\n",
    "                    if \"args\" in val:\n",
    "                        if \"name\" in val[\"args\"]:\n",
    "                            d[\"name\"] = val[\"args\"][\"name\"]\n",
    "                        if \"value\" in val[\"args\"]:\n",
    "                            d[\"hash\"] = val[\"args\"][\"value\"]\n",
    "                elif val[\"name\"] == \"HH\":\n",
    "                    d[\"type\"] = 2\n",
    "                    if \"args\" in val:\n",
    "                        if \"name\" in val[\"args\"]:\n",
    "                            d[\"name\"] = val[\"args\"][\"name\"]\n",
    "                        if \"value\" in val[\"args\"]:\n",
    "                            d[\"hash\"] = val[\"args\"][\"value\"]\n",
    "            \n",
    "        except Exception as error:\n",
    "            logging.error(f\"Processing {line} failed with {error}\")\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dask Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 12:37:37,416 [INFO]: Created index for 6 files in /var/tmp/haridev/ipykernel_359281/864066620.py:3\n",
      "2024-11-10 12:37:37,533 [INFO]: Total size of all files are <dask.bag.core.Item object at 0x15552128afd0> bytes in /var/tmp/haridev/ipykernel_359281/864066620.py:6\n",
      "2024-11-10 12:37:37,588 [INFO]: Max lines per file are [('/usr/workspace/haridev/xio//logs/ior/ior_4k.pfw.gz', 7565244), ('/usr/workspace/haridev/xio//logs/ior/ior_16k-64k.pfw.gz', 9788674), ('/usr/workspace/haridev/xio//logs/ior/ior-1mb-4mb.pfw.gz', 34785028), ('/usr/workspace/haridev/xio//logs/ior/ior-256k.pfw.gz', 9397525), ('/usr/workspace/haridev/xio//logs/ior/ior-256k-multiple-interfaces.pfw.gz', 60400779), ('/usr/workspace/haridev/xio//logs/ior/ior-hdf5-256k.pfw.gz', 25153457)] in /var/tmp/haridev/ipykernel_359281/864066620.py:8\n",
      "2024-11-10 12:37:37,615 [INFO]: Loading 8981 batches out of 6 files and has 147090707 lines overall in /var/tmp/haridev/ipykernel_359281/864066620.py:16\n"
     ]
    }
   ],
   "source": [
    "if len(file_pattern) > 0:\n",
    "    dask.bag.from_sequence(file_pattern).map(create_index).compute()\n",
    "    logging.info(f\"Created index for {len(file_pattern)} files\")\n",
    "    total_size = dask.bag.from_sequence(file_pattern).map(get_size).sum()\n",
    "    n_partition = math.ceil(total_size.compute() / (128 * 1024 ** 2))\n",
    "    logging.info(f\"Total size of all files are {total_size} bytes\")\n",
    "    max_line_numbers = dask.bag.from_sequence(file_pattern).map(get_linenumber).compute()\n",
    "    logging.info(f\"Max lines per file are {max_line_numbers}\")\n",
    "    json_line_delayed = []\n",
    "    total_lines = 0\n",
    "    for filename, max_line in max_line_numbers:\n",
    "        total_lines += max_line\n",
    "        for _, start, end in generate_line_batches(filename, max_line):\n",
    "            json_line_delayed.append((filename, start, end))\n",
    "\n",
    "    logging.info(f\"Loading {len(json_line_delayed)} batches out of {len(file_pattern)} files and has {total_lines} lines overall\")\n",
    "    json_line_bags = []\n",
    "    for filename, start, end in json_line_delayed:\n",
    "        num_lines = end - start + 1\n",
    "        json_line_bags.append(dask.delayed(load_indexed_gzip_files, nout=num_lines)(filename, start, end))\n",
    "    json_lines = dask.bag.concat(json_line_bags)\n",
    "    if is_trace:\n",
    "        pfw_bag = json_lines.map(load_trace).filter(lambda x: \"name\" in x)\n",
    "    else:\n",
    "        pfw_bag = json_lines.map(load_profile).filter(lambda x: \"func_id\" in x)\n",
    "    pfw_bag.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_trace:\n",
    "    columns = {'hhash': \"string[pyarrow]\", 'pid': \"uint64[pyarrow]\", 'tid': \"uint64[pyarrow]\",\n",
    "                'cat': \"string[pyarrow]\", 'name': \"string[pyarrow]\", 'type':  \"uint8[pyarrow]\",\n",
    "            'ts': \"uint64[pyarrow]\", 'te': \"uint64[pyarrow]\", 'dur': \"uint64[pyarrow]\", 'interval': \"string[pyarrow]\", \n",
    "             'size': \"uint64[pyarrow]\", 'fhash': \"string[pyarrow]\", 'hash': \"string[pyarrow]\", \n",
    "           }\n",
    "else:\n",
    "    columns = {'pid': \"uint64[pyarrow]\", 'tid': \"uint64[pyarrow]\",\n",
    "            'ts_us': \"uint64[pyarrow]\", 'dur_sec': \"float32[pyarrow]\", \n",
    "            'freq': \"uint64[pyarrow]\", 'size_bytes': \"uint64[pyarrow]\", 'name': \"string[pyarrow]\", \n",
    "            'filename': \"string[pyarrow]\", \n",
    "            'cat': \"string[pyarrow]\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = pfw_bag.to_dataframe(meta=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = events.repartition(npartitions=n_partition).persist()\n",
    "_ = wait(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhash = events.query(\"type == 1\")[[\"name\",\"hash\"]]\n",
    "hhash = events.query(\"type == 2\")[[\"name\",\"hash\"]]\n",
    "event = events.query(\"type == 0\")\n",
    "fhashes = fhash.query(\"name.str.contains('test.bat')\").compute()[\"hash\"]\n",
    "fhashes = fhashes.to_list()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hhash</th>\n",
       "      <th>pid</th>\n",
       "      <th>tid</th>\n",
       "      <th>cat</th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>ts</th>\n",
       "      <th>te</th>\n",
       "      <th>dur</th>\n",
       "      <th>interval</th>\n",
       "      <th>size</th>\n",
       "      <th>fhash</th>\n",
       "      <th>hash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3368</th>\n",
       "      <td>ecd9cccc050c9e893ab33b1a228fe76d</td>\n",
       "      <td>15882</td>\n",
       "      <td>15882</td>\n",
       "      <td>sys</td>\n",
       "      <td>openat</td>\n",
       "      <td>0</td>\n",
       "      <td>5448559896</td>\n",
       "      <td>5448937723</td>\n",
       "      <td>377827</td>\n",
       "      <td>[5448559896,5448937723)</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>3c30b6298abfeb121cd313effdb92256</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3374</th>\n",
       "      <td>ecd9cccc050c9e893ab33b1a228fe76d</td>\n",
       "      <td>15882</td>\n",
       "      <td>15882</td>\n",
       "      <td>sys</td>\n",
       "      <td>lseek</td>\n",
       "      <td>0</td>\n",
       "      <td>5448956059</td>\n",
       "      <td>5448965554</td>\n",
       "      <td>9495</td>\n",
       "      <td>[5448956059,5448965554)</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>3c30b6298abfeb121cd313effdb92256</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3609</th>\n",
       "      <td>ecd9cccc050c9e893ab33b1a228fe76d</td>\n",
       "      <td>15882</td>\n",
       "      <td>15882</td>\n",
       "      <td>sys</td>\n",
       "      <td>write</td>\n",
       "      <td>0</td>\n",
       "      <td>5448970686</td>\n",
       "      <td>5449450628</td>\n",
       "      <td>479942</td>\n",
       "      <td>[5448970686,5449450628)</td>\n",
       "      <td>16384</td>\n",
       "      <td>3c30b6298abfeb121cd313effdb92256</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3613</th>\n",
       "      <td>ecd9cccc050c9e893ab33b1a228fe76d</td>\n",
       "      <td>15882</td>\n",
       "      <td>15882</td>\n",
       "      <td>sys</td>\n",
       "      <td>lseek</td>\n",
       "      <td>0</td>\n",
       "      <td>5449469108</td>\n",
       "      <td>5449473314</td>\n",
       "      <td>4206</td>\n",
       "      <td>[5449469108,5449473314)</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>3c30b6298abfeb121cd313effdb92256</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3858</th>\n",
       "      <td>ecd9cccc050c9e893ab33b1a228fe76d</td>\n",
       "      <td>15882</td>\n",
       "      <td>15882</td>\n",
       "      <td>sys</td>\n",
       "      <td>write</td>\n",
       "      <td>0</td>\n",
       "      <td>5449477719</td>\n",
       "      <td>5449874977</td>\n",
       "      <td>397258</td>\n",
       "      <td>[5449477719,5449874977)</td>\n",
       "      <td>16384</td>\n",
       "      <td>3c30b6298abfeb121cd313effdb92256</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>ecd9cccc050c9e893ab33b1a228fe76d</td>\n",
       "      <td>252574</td>\n",
       "      <td>252574</td>\n",
       "      <td>sys</td>\n",
       "      <td>pread64</td>\n",
       "      <td>0</td>\n",
       "      <td>7715452830316</td>\n",
       "      <td>7715453991341</td>\n",
       "      <td>1161025</td>\n",
       "      <td>[7715452830316,7715453991341)</td>\n",
       "      <td>262144</td>\n",
       "      <td>1.438620452325829e+19</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2302</th>\n",
       "      <td>ecd9cccc050c9e893ab33b1a228fe76d</td>\n",
       "      <td>252574</td>\n",
       "      <td>252574</td>\n",
       "      <td>sys</td>\n",
       "      <td>pread64</td>\n",
       "      <td>0</td>\n",
       "      <td>7715729832783</td>\n",
       "      <td>7715736439761</td>\n",
       "      <td>6606978</td>\n",
       "      <td>[7715729832783,7715736439761)</td>\n",
       "      <td>262144</td>\n",
       "      <td>1.438620452325829e+19</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4070</th>\n",
       "      <td>ecd9cccc050c9e893ab33b1a228fe76d</td>\n",
       "      <td>252574</td>\n",
       "      <td>252574</td>\n",
       "      <td>sys</td>\n",
       "      <td>pread64</td>\n",
       "      <td>0</td>\n",
       "      <td>7715952253337</td>\n",
       "      <td>7715955873221</td>\n",
       "      <td>3619884</td>\n",
       "      <td>[7715952253337,7715955873221)</td>\n",
       "      <td>262144</td>\n",
       "      <td>1.438620452325829e+19</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4437</th>\n",
       "      <td>ecd9cccc050c9e893ab33b1a228fe76d</td>\n",
       "      <td>252574</td>\n",
       "      <td>252574</td>\n",
       "      <td>sys</td>\n",
       "      <td>pread64</td>\n",
       "      <td>0</td>\n",
       "      <td>7715998108265</td>\n",
       "      <td>7716001726245</td>\n",
       "      <td>3617980</td>\n",
       "      <td>[7715998108265,7716001726245)</td>\n",
       "      <td>262144</td>\n",
       "      <td>1.438620452325829e+19</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6850</th>\n",
       "      <td>ecd9cccc050c9e893ab33b1a228fe76d</td>\n",
       "      <td>252574</td>\n",
       "      <td>252574</td>\n",
       "      <td>sys</td>\n",
       "      <td>pread64</td>\n",
       "      <td>0</td>\n",
       "      <td>7716303105978</td>\n",
       "      <td>7716304259171</td>\n",
       "      <td>1153193</td>\n",
       "      <td>[7716303105978,7716304259171)</td>\n",
       "      <td>262144</td>\n",
       "      <td>1.438620452325829e+19</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>853152 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 hhash     pid     tid  cat     name  type  \\\n",
       "3368  ecd9cccc050c9e893ab33b1a228fe76d   15882   15882  sys   openat     0   \n",
       "3374  ecd9cccc050c9e893ab33b1a228fe76d   15882   15882  sys    lseek     0   \n",
       "3609  ecd9cccc050c9e893ab33b1a228fe76d   15882   15882  sys    write     0   \n",
       "3613  ecd9cccc050c9e893ab33b1a228fe76d   15882   15882  sys    lseek     0   \n",
       "3858  ecd9cccc050c9e893ab33b1a228fe76d   15882   15882  sys    write     0   \n",
       "...                                ...     ...     ...  ...      ...   ...   \n",
       "60    ecd9cccc050c9e893ab33b1a228fe76d  252574  252574  sys  pread64     0   \n",
       "2302  ecd9cccc050c9e893ab33b1a228fe76d  252574  252574  sys  pread64     0   \n",
       "4070  ecd9cccc050c9e893ab33b1a228fe76d  252574  252574  sys  pread64     0   \n",
       "4437  ecd9cccc050c9e893ab33b1a228fe76d  252574  252574  sys  pread64     0   \n",
       "6850  ecd9cccc050c9e893ab33b1a228fe76d  252574  252574  sys  pread64     0   \n",
       "\n",
       "                 ts             te      dur                       interval  \\\n",
       "3368     5448559896     5448937723   377827        [5448559896,5448937723)   \n",
       "3374     5448956059     5448965554     9495        [5448956059,5448965554)   \n",
       "3609     5448970686     5449450628   479942        [5448970686,5449450628)   \n",
       "3613     5449469108     5449473314     4206        [5449469108,5449473314)   \n",
       "3858     5449477719     5449874977   397258        [5449477719,5449874977)   \n",
       "...             ...            ...      ...                            ...   \n",
       "60    7715452830316  7715453991341  1161025  [7715452830316,7715453991341)   \n",
       "2302  7715729832783  7715736439761  6606978  [7715729832783,7715736439761)   \n",
       "4070  7715952253337  7715955873221  3619884  [7715952253337,7715955873221)   \n",
       "4437  7715998108265  7716001726245  3617980  [7715998108265,7716001726245)   \n",
       "6850  7716303105978  7716304259171  1153193  [7716303105978,7716304259171)   \n",
       "\n",
       "        size                             fhash  hash  \n",
       "3368    <NA>  3c30b6298abfeb121cd313effdb92256  <NA>  \n",
       "3374    <NA>  3c30b6298abfeb121cd313effdb92256  <NA>  \n",
       "3609   16384  3c30b6298abfeb121cd313effdb92256  <NA>  \n",
       "3613    <NA>  3c30b6298abfeb121cd313effdb92256  <NA>  \n",
       "3858   16384  3c30b6298abfeb121cd313effdb92256  <NA>  \n",
       "...      ...                               ...   ...  \n",
       "60    262144             1.438620452325829e+19  <NA>  \n",
       "2302  262144             1.438620452325829e+19  <NA>  \n",
       "4070  262144             1.438620452325829e+19  <NA>  \n",
       "4437  262144             1.438620452325829e+19  <NA>  \n",
       "6850  262144             1.438620452325829e+19  <NA>  \n",
       "\n",
       "[853152 rows x 13 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "interesting_events = event.query(\"fhash.isin(@value)\", local_dict={\"value\": fhashes}).sort_values(\"ts\")\n",
    "interesting_events.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/workspace/haridev/datacrumbs/venv/lib/python3.9/site-packages/dask_expr/_shuffle.py:1394: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  mins = mins.bfill()\n",
      "/usr/workspace/haridev/datacrumbs/venv/lib/python3.9/site-packages/dask_expr/_shuffle.py:1395: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  maxes = maxes.bfill()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 12:48:52,787 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 37391b081af65c2e45cbae88ab710187 initialized by task ('shuffle-transfer-37391b081af65c2e45cbae88ab710187', 131) executed on worker tcp://127.0.0.1:37081\n",
      "2024-11-10 12:48:56,682 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 37391b081af65c2e45cbae88ab710187 deactivated due to stimulus 'task-finished-1731271736.6801343'\n",
      "2024-11-10 12:49:01,471 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle dc3f970979eefcc3a5816445870e8dec initialized by task ('shuffle-transfer-dc3f970979eefcc3a5816445870e8dec', 115) executed on worker tcp://127.0.0.1:37481\n",
      "2024-11-10 12:49:04,341 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle dc3f970979eefcc3a5816445870e8dec deactivated due to stimulus 'task-finished-1731271744.3392837'\n",
      "2024-11-10 12:49:09,948 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle dbe2eb22ff8fe1d84f492ee06d99a2f2 initialized by task ('shuffle-transfer-dbe2eb22ff8fe1d84f492ee06d99a2f2', 107) executed on worker tcp://127.0.0.1:40355\n",
      "2024-11-10 12:50:23,323 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle dbe2eb22ff8fe1d84f492ee06d99a2f2 deactivated due to stimulus 'task-finished-1731271820.2580884'\n",
      "2024-11-10 13:10:44,574 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:42569 (pid=359475) exceeded 95% memory budget. Restarting...\n",
      "2024-11-10 13:10:46,564 - distributed.scheduler - ERROR - Couldn't gather keys: {('repartitiontofewer-485467e32aa272cee3e20d60ed366f46', 0): 'forgotten'}\n",
      "2024-11-10 13:10:51,640 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:42569' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('repartitiontofewer-ae258baaee3a3964352a8af406c1ba0f', 99), ('repartitiontofewer-ae258baaee3a3964352a8af406c1ba0f', 224), ('repartitiontofewer-ae258baaee3a3964352a8af406c1ba0f', 102), ('repartitiontofewer-ae258baaee3a3964352a8af406c1ba0f', 233), ('repartitiontofewer-ae258baaee3a3964352a8af406c1ba0f', 65), ('repartitiontofewer-ae258baaee3a3964352a8af406c1ba0f', 248), ('repartitiontofewer-ae258baaee3a3964352a8af406c1ba0f', 187), ('repartitiontofewer-ae258baaee3a3964352a8af406c1ba0f', 141), ('repartitiontofewer-ae258baaee3a3964352a8af406c1ba0f', 31), ('repartitiontofewer-ae258baaee3a3964352a8af406c1ba0f', 165), ('repartitiontofewer-ae258baaee3a3964352a8af406c1ba0f', 110), ('repartitiontofewer-ae258baaee3a3964352a8af406c1ba0f', 119), ('repartitiontofewer-ae258baaee3a3964352a8af406c1ba0f', 238), ('repartitiontofewer-ae258baaee3a3964352a8af406c1ba0f', 58), ('repartitiontofewer-ae258baaee3a3964352a8af406c1ba0f', 195), ('repartitiontofewer-ae258baaee3a3964352a8af406c1ba0f', 253), ('repartitiontofewer-ae258baaee3a3964352a8af406c1ba0f', 155), ('repartitiontofewer-ae258baaee3a3964352a8af406c1ba0f', 274), ('repartitiontofewer-ae258baaee3a3964352a8af406c1ba0f', 100), ('repartitiontofewer-ae258baaee3a3964352a8af406c1ba0f', 127), ('repartitiontofewer-ae258baaee3a3964352a8af406c1ba0f', 5), ('repartitiontofewer-ae258baaee3a3964352a8af406c1ba0f', 255), ('repartitiontofewer-ae258baaee3a3964352a8af406c1ba0f', 209), ('repartitiontofewer-ae258baaee3a3964352a8af406c1ba0f', 148)} (stimulus_id='handle-worker-cleanup-1731273051.6402755')\n",
      "2024-11-10 13:10:51,872 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-11-10 13:11:08,221 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 992cdf4445c9ef9ded9e534b91389564 initialized by task ('shuffle-transfer-992cdf4445c9ef9ded9e534b91389564', 38) executed on worker tcp://127.0.0.1:44841\n",
      "2024-11-10 13:11:55,561 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 992cdf4445c9ef9ded9e534b91389564 deactivated due to stimulus 'task-finished-1731273115.5558321'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0              1\n",
       "1              1\n",
       "2          16384\n",
       "3              1\n",
       "4          16384\n",
       "           ...  \n",
       "853147    262144\n",
       "853148    262144\n",
       "853149    262144\n",
       "853150    262144\n",
       "853151    262144\n",
       "Name: size, Length: 853152, dtype: uint64[pyarrow]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interesting_events[\"combined_name\"] = interesting_events[\"name\"] + \"-\" + interesting_events[\"cat\"]\n",
    "ts_events = interesting_events[[\"size\"]].compute().reset_index().drop(\"index\", axis=1)\n",
    "ts_events[\"size\"].fillna(value=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/workspace/haridev/datacrumbs/venv/lib/python3.9/site-packages/dask_expr/_shuffle.py:1394: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  mins = mins.bfill()\n",
      "/usr/workspace/haridev/datacrumbs/venv/lib/python3.9/site-packages/dask_expr/_shuffle.py:1395: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  maxes = maxes.bfill()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interval</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3368</th>\n",
       "      <td>[5448559896,5448937723)</td>\n",
       "      <td>openat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3374</th>\n",
       "      <td>[5448956059,5448965554)</td>\n",
       "      <td>lseek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3609</th>\n",
       "      <td>[5448970686,5449450628)</td>\n",
       "      <td>write</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3613</th>\n",
       "      <td>[5449469108,5449473314)</td>\n",
       "      <td>lseek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3858</th>\n",
       "      <td>[5449477719,5449874977)</td>\n",
       "      <td>write</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>[7715452830316,7715453991341)</td>\n",
       "      <td>pread64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2302</th>\n",
       "      <td>[7715729832783,7715736439761)</td>\n",
       "      <td>pread64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4070</th>\n",
       "      <td>[7715952253337,7715955873221)</td>\n",
       "      <td>pread64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4437</th>\n",
       "      <td>[7715998108265,7716001726245)</td>\n",
       "      <td>pread64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6850</th>\n",
       "      <td>[7716303105978,7716304259171)</td>\n",
       "      <td>pread64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>853152 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           interval     name\n",
       "3368        [5448559896,5448937723)   openat\n",
       "3374        [5448956059,5448965554)    lseek\n",
       "3609        [5448970686,5449450628)    write\n",
       "3613        [5449469108,5449473314)    lseek\n",
       "3858        [5449477719,5449874977)    write\n",
       "...                             ...      ...\n",
       "60    [7715452830316,7715453991341)  pread64\n",
       "2302  [7715729832783,7715736439761)  pread64\n",
       "4070  [7715952253337,7715955873221)  pread64\n",
       "4437  [7715998108265,7716001726245)  pread64\n",
       "6850  [7716303105978,7716304259171)  pread64\n",
       "\n",
       "[853152 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interesting_intervals  = interesting_events[[\"interval\",\"name\"]].compute()\n",
    "interesting_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# interesting_events[\"interval\"] = interesting_events.apply(lambda x: I.to_string(I.closed(x[\"ts\"], x[\"ts\"]+x[\"dur\"])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/workspace/haridev/datacrumbs/venv/lib/python3.9/site-packages/dask_expr/_shuffle.py:1394: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  mins = mins.bfill()\n",
      "/usr/workspace/haridev/datacrumbs/venv/lib/python3.9/site-packages/dask_expr/_shuffle.py:1395: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  maxes = maxes.bfill()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([[5448559896,5448937723),\n",
       "  [5448956059,5448965554),\n",
       "  [5448970686,5449450628),\n",
       "  [5449469108,5449473314),\n",
       "  [5449477719,5449874977),\n",
       "  [5449884999,5449888787),\n",
       "  [5449892961,5450252184),\n",
       "  [5450261489,5450264918),\n",
       "  [5450268966,5450631486),\n",
       "  [5450640553,5450644029)],\n",
       " 422948)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def group_func(df):\n",
    "    val = I.empty()\n",
    "    for index, value in df.items():\n",
    "        if str(value) != 'NA':\n",
    "            pad_interval = I.from_string(str(value), int)\n",
    "            val = val.union(pad_interval)\n",
    "    logging.debug(f\"Grouped Range into {val}\")\n",
    "    return I.to_string(val)\n",
    "def union_portions():\n",
    "    return dd.Aggregation(\n",
    "        'union_portions',\n",
    "        chunk=lambda s: s.apply(group_func),\n",
    "        agg=lambda s: s.apply(group_func)\n",
    "    )\n",
    "relevant_intervals = interesting_events[[\"interval\"]].reduction(chunk=lambda s: s.apply(group_func), aggregate=lambda s1: s1.apply(group_func))[\"interval\"].compute()\n",
    "relevant_intervals = I.from_string(relevant_intervals, int)\n",
    "relevant_intervals_list = list(relevant_intervals)\n",
    "relevant_intervals_list[:10], len(relevant_intervals_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5448559896, 7716304259171)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_ts = relevant_intervals_list[0].lower\n",
    "max_te = relevant_intervals_list[-1].upper\n",
    "min_ts, max_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_events = event.query(f\"ts >= {min_ts - 1e5} and te <= {max_te + 1e5} and dur > 0\")\n",
    "# filtered_events = event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_events[\"combined_name\"] = filtered_events[\"name\"] + \"-\" + filtered_events[\"cat\"]\n",
    "# event_batch_per_sys_call = set()\n",
    "# rows = list(interesting_intervals.iterrows())\n",
    "# count = 0\n",
    "# ops_map = {}\n",
    "# ops_counter = 0\n",
    "# for index, row in tqdm(rows):\n",
    "#     interval = I.from_string(row[\"interval\"], int)\n",
    "#     ops = row[\"name\"]\n",
    "#     a_overlaps_b = f\"(ts >= {interval.lower} and ts <=  {interval.upper}) or (te >= {interval.lower} and te <=  {interval.upper})\"\n",
    "#     b_overlaps_a = f\"({interval.lower} >= ts and {interval.lower} <=  te) or ({interval.upper} >= ts and {interval.upper} <=  te)\"\n",
    "#     batch = filtered_events.query(f\"{a_overlaps_b} or {b_overlaps_a}\")[\"combined_name\"].unique().compute()\n",
    "#     event_batch_per_sys_call.update(batch)\n",
    "#     count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 13:11:43,068 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name=\"execute(('concat-filter-lambda-load_indexed_gzip_files-load_trace-to_dataframe-c79a9f1fc3c9b5f0e9f19de3114f3631', 4529))\" coro=<Worker.execute() done, defined at /usr/workspace/haridev/datacrumbs/venv/lib/python3.9/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError\n",
      "2024-11-10 13:11:43,098 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name=\"execute(('concat-filter-lambda-load_indexed_gzip_files-load_trace-to_dataframe-c79a9f1fc3c9b5f0e9f19de3114f3631', 4528))\" coro=<Worker.execute() done, defined at /usr/workspace/haridev/datacrumbs/venv/lib/python3.9/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError\n",
      "2024-11-10 13:11:43,098 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name=\"execute(('concat-filter-lambda-load_indexed_gzip_files-load_trace-to_dataframe-c79a9f1fc3c9b5f0e9f19de3114f3631', 4530))\" coro=<Worker.execute() done, defined at /usr/workspace/haridev/datacrumbs/venv/lib/python3.9/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError\n",
      "2024-11-10 13:11:43,769 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/workspace/haridev/datacrumbs/venv/lib/python3.9/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/usr/workspace/haridev/datacrumbs/venv/lib/python3.9/site-packages/distributed/comm/core.py\", line 342, in connect\n",
      "    comm = await wait_for(\n",
      "  File \"/usr/workspace/haridev/datacrumbs/venv/lib/python3.9/site-packages/distributed/utils.py\", line 1957, in wait_for\n",
      "    return await asyncio.wait_for(fut, timeout)\n",
      "  File \"/collab/usr/gapps/python/build/spack-toss4.1/var/spack/environments/python/._view/75prb56irmif5ejtirjthpx6kq3gqo52/lib/python3.9/asyncio/tasks.py\", line 466, in wait_for\n",
      "    await waiter\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/workspace/haridev/datacrumbs/venv/lib/python3.9/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/workspace/haridev/datacrumbs/venv/lib/python3.9/site-packages/distributed/worker.py\", line 1250, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/usr/workspace/haridev/datacrumbs/venv/lib/python3.9/site-packages/distributed/utils_comm.py\", line 459, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/usr/workspace/haridev/datacrumbs/venv/lib/python3.9/site-packages/distributed/utils_comm.py\", line 438, in retry\n",
      "    return await coro()\n",
      "  File \"/usr/workspace/haridev/datacrumbs/venv/lib/python3.9/site-packages/distributed/core.py\", line 1251, in send_recv_from_rpc\n",
      "    comm = await self.pool.connect(self.addr)\n",
      "  File \"/usr/workspace/haridev/datacrumbs/venv/lib/python3.9/site-packages/distributed/core.py\", line 1535, in connect\n",
      "    raise CommClosedError(reason)\n",
      "distributed.comm.core.CommClosedError: ConnectionPool closing.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>dur</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">kernel</th>\n",
       "      <th>ext4_get_inode_loc</th>\n",
       "      <td>1252881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ext4_set_aops</th>\n",
       "      <td>647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ext4_create</th>\n",
       "      <td>533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ext4_da_get_block_prep</th>\n",
       "      <td>1239086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>__mnt_want_write_file</th>\n",
       "      <td>102686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">hdf5</th>\n",
       "      <th>H5C_flush_cache</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H5C_apply_candidate_list</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H5ES_term_package</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H5FD_locate_signature</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H5S_mpio_space_type</th>\n",
       "      <td>2108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1356 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     dur\n",
       "cat    name                             \n",
       "kernel ext4_get_inode_loc        1252881\n",
       "       ext4_set_aops                 647\n",
       "       ext4_create                   533\n",
       "       ext4_da_get_block_prep    1239086\n",
       "       __mnt_want_write_file      102686\n",
       "...                                  ...\n",
       "hdf5   H5C_flush_cache                 1\n",
       "       H5C_apply_candidate_list        1\n",
       "       H5ES_term_package               1\n",
       "       H5FD_locate_signature          15\n",
       "       H5S_mpio_space_type          2108\n",
       "\n",
       "[1356 rows x 1 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_functions = filtered_events.groupby([\"cat\",\"name\"]).agg({\"dur\":\"count\"}).compute()\n",
    "unique_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['kernel', 'sys', 'os_cache', 'vfs'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_cat = {}\n",
    "ignore_user_categories = ['hdf5', 'c', 'mpi', \"sys\"]\n",
    "for cat, value in unique_functions.index:\n",
    "    if cat not in ignore_user_categories:\n",
    "        if cat not in function_cat:\n",
    "            function_cat[cat]=set()\n",
    "        function_cat[cat].add(value)\n",
    "function_cat.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def set_default(obj):\n",
    "    if isinstance(obj, set):\n",
    "        return list(obj)\n",
    "    raise TypeError\n",
    "function_file = f\"{app_root}/datacrumbs/configs/function.json\"\n",
    "with open(function_file, 'w') as convert_file: \n",
    "     convert_file.write(json.dumps(function_cat, default=set_default))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 11:59:09,448 [ERROR]: Task exception was never retrieved\n",
      "future: <Task finished name='Task-10104337' coro=<Client._gather.<locals>.wait() done, defined at /usr/workspace/haridev/datacrumbs/venv/lib/python3.9/site-packages/distributed/client.py:2391> exception=AllExit()> in /collab/usr/gapps/python/build/spack-toss4.1/var/spack/environments/python/._view/75prb56irmif5ejtirjthpx6kq3gqo52/lib/python3.9/asyncio/base_events.py:1753\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/workspace/haridev/datacrumbs/venv/lib/python3.9/site-packages/distributed/client.py\", line 2400, in wait\n",
      "    raise AllExit()\n",
      "distributed.client.AllExit\n",
      "/usr/workspace/haridev/datacrumbs/venv/lib/python3.9/site-packages/distributed/client.py:3357: UserWarning: Sending large graph of size 863.67 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batch_columns_list = dask.compute(event_batch_per_sys_call)\n",
    "batch_columns_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = None\n",
    "count = 0\n",
    "for batch in tqdm(event_batch_per_sys_call):\n",
    "    if merged_df is not None:\n",
    "        merged_df = merged_df.merge(batch, how='outer', on=\"combined_name\",suffixes=('', f\"_{count}\"))\n",
    "    else:\n",
    "        merged_df = batch\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = merged_df.transpose()\n",
    "df = dataset.reset_index().drop(\"index\", axis=1)\n",
    "df[\"op_name\"] = \"UNKNOWN\"\n",
    "for key, value in ops_map.items():\n",
    "    df[\"op_name\"] = df[\"op_name\"].mask(df[\"op\"].eq(value), key)\n",
    "df[\"op_name\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = df.merge(ts_events, how='outer', left_index=True, right_index=True)\n",
    "final_dataset[\"BW\"] = final_dataset[f\"size\"] / (1024**2) / (final_dataset[f\"{ops}-sys\"] / 1e9)\n",
    "final_dataset.drop([f\"{ops}-sys\", \"size\"], inplace=True, axis=1)\n",
    "final_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file\n",
    "final_dataset[\"op\"] = ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset.to_json(path_or_buf=f\"{output_file}\",orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file=\"/usr/workspace/haridev/xio/output/jslines/write_ops-64_ts-64m-RAW-DIRECT.pfw.gz.jsonl\"\n",
    "ops=\"write\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "files = glob(f\"{output_file}\")\n",
    "final_dataset_l = []\n",
    "for file in files:\n",
    "    final_dataset_l.append(dd.read_json(file))\n",
    "final_dataset = dd.concat(final_dataset_l).compute().reset_index().drop(\"index\", axis=1)\n",
    "final_dataset[\"BW\"] = final_dataset[f\"transfer_size\"] / (1024**2) / (final_dataset[f\"{ops}-sys\"]/1e9)\n",
    "final_dataset[\"op\"] = ops\n",
    "final_dataset.to_json(path_or_buf=f\"{output_file}\",orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make the number of relevance features dynamic.\n",
    "\n",
    "1. Add up the importance score to reach 95%.\n",
    "2. Add Transfer size\n",
    "3. Split features into layers and do this analysis per layer.\n",
    "4. Correlation\n",
    "   1. correlation matrix.\n",
    "   2. PCA\n",
    "   3. Lasso Regression (L1)\n",
    "   4. Auto regression\n",
    "5. SHAPLEY value (feature importance)\n",
    "   1. Tree SHAP\n",
    "6. How portable are the interfaces (do not overfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tanzima for better models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## models\n",
    "- sequential training: gradient boost\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
